{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes a sequence of numbers as input (e.g., [1, 2, 3]) and predicts the reversed sequence (e.g., [3, 2, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DSI_ISE\\dsiis_env\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor', 'keras_tensor_4']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3053 - loss: 2.0117\n",
      "Epoch 2/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6374 - loss: 0.9919\n",
      "Epoch 3/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9539 - loss: 0.3568\n",
      "Epoch 4/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9945 - loss: 0.1451\n",
      "Epoch 5/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9988 - loss: 0.0760\n",
      "Epoch 6/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9997 - loss: 0.0465\n",
      "Epoch 7/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0305\n",
      "Epoch 8/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0214\n",
      "Epoch 9/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0155\n",
      "Epoch 10/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0123\n",
      "Epoch 11/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0095\n",
      "Epoch 12/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0077\n",
      "Epoch 13/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0062\n",
      "Epoch 14/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0051\n",
      "Epoch 15/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0043\n",
      "Epoch 16/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 17/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 18/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 1.0000 - loss: 0.0026\n",
      "Epoch 19/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 20/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0019\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DSI_ISE\\dsiis_env\\lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_4', 'keras_tensor_15', 'keras_tensor_13', 'keras_tensor_14']. Received: the structure of inputs=('*', '*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Input sequence: [5 4 8 6 9]\n",
      "Decoded sequence: [np.int64(9), np.int64(6), np.int64(8), np.int64(4), np.int64(5)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dot, Activation, Concatenate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the data\n",
    "num_samples = 10000  # Number of sequences\n",
    "input_length = 5     # Length of each input sequence\n",
    "input_vocab_size = 10  # Vocabulary size (digits 0-9)\n",
    "\n",
    "# Generate random input sequences\n",
    "input_sequences = np.random.randint(1, input_vocab_size, (num_samples, input_length))\n",
    "target_sequences = np.flip(input_sequences, axis=1)  # Reverse the sequences for the target\n",
    "\n",
    "# One-hot encode the input and target sequences\n",
    "input_onehot = np.eye(input_vocab_size)[input_sequences]\n",
    "target_onehot = np.eye(input_vocab_size)[target_sequences]\n",
    "\n",
    "# Prepare decoder input sequences (shifted target sequences)\n",
    "decoder_input_sequences = np.zeros_like(target_onehot)\n",
    "decoder_input_sequences[:, 1:, :] = target_onehot[:, :-1, :]  # Shift target sequences to the right\n",
    "decoder_input_sequences[:, 0, :] = 0  # Set <start> token as all zeros\n",
    "\n",
    "# Define the Encoder\n",
    "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
    "encoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Define the Decoder\n",
    "decoder_inputs = Input(shape=(None, input_vocab_size))\n",
    "decoder_lstm = LSTM(64, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "# Attention Mechanism\n",
    "attention = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])  # Compute similarity scores\n",
    "attention = Activation('softmax')(attention)  # Normalize scores to probabilities\n",
    "context = Dot(axes=[2, 1])([attention, encoder_outputs])  # Weighted sum of encoder outputs\n",
    "\n",
    "# Concatenate context vector with decoder LSTM outputs\n",
    "decoder_combined_context = Concatenate(axis=-1)([context, decoder_outputs])\n",
    "\n",
    "# Final output layer\n",
    "decoder_dense = Dense(input_vocab_size, activation='softmax')\n",
    "final_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Define the full model\n",
    "model = Model([encoder_inputs, decoder_inputs], final_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([input_onehot, decoder_input_sequences], target_onehot, epochs=20, batch_size=64, verbose=1)\n",
    "\n",
    "# Define the inference models\n",
    "# Encoder inference model\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(64,))\n",
    "decoder_state_input_c = Input(shape=(64,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_hidden_state_input = Input(shape=(input_length, 64))  # Encoder outputs\n",
    "\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "attention = Dot(axes=[2, 2])([decoder_lstm_outputs, decoder_hidden_state_input])\n",
    "attention = Activation('softmax')(attention)\n",
    "context = Dot(axes=[2, 1])([attention, decoder_hidden_state_input])\n",
    "decoder_combined_context = Concatenate(axis=-1)([context, decoder_lstm_outputs])\n",
    "decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs, decoder_hidden_state_input] + decoder_states_inputs,\n",
    "    [decoder_outputs, attention, state_h, state_c]\n",
    ")\n",
    "\n",
    "# Function to decode a sequence\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input sequence\n",
    "    encoder_outputs, state_h, state_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Create an empty target sequence with a <start> token (all zeros)\n",
    "    target_seq = np.zeros((1, 1, input_vocab_size))\n",
    "\n",
    "    # Initialize the decoded sequence\n",
    "    decoded_sequence = []\n",
    "    attention_weights = []\n",
    "\n",
    "    # Generate tokens one-by-one\n",
    "    for _ in range(input_length):\n",
    "        # Predict the next token\n",
    "        output_tokens, att_weights, h, c = decoder_model.predict(\n",
    "            [target_seq, encoder_outputs, state_h, state_c]\n",
    "        )\n",
    "\n",
    "        # Save attention weights for visualization\n",
    "        attention_weights.append(att_weights[0])\n",
    "\n",
    "        # Get the token with the highest probability\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        decoded_sequence.append(sampled_token_index)\n",
    "\n",
    "        # Update the target sequence (input to the next step of the decoder)\n",
    "        target_seq = np.zeros((1, 1, input_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1\n",
    "\n",
    "        # Update the decoder's states\n",
    "        state_h, state_c = h, c\n",
    "\n",
    "    return decoded_sequence, np.array(attention_weights)\n",
    "\n",
    "# Visualize attention weights\n",
    "def plot_attention(input_seq, attention_weights, decoded_seq):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    attention_weights = attention_weights.squeeze()\n",
    "    ax.matshow(attention_weights, cmap='viridis')\n",
    "    ax.set_xticks(range(len(input_seq[0])))\n",
    "    ax.set_yticks(range(len(decoded_seq)))\n",
    "    ax.set_xticklabels(input_seq[0], rotation=45)\n",
    "    ax.set_yticklabels(decoded_seq)\n",
    "    plt.show()\n",
    "\n",
    "# Test the model with a new input\n",
    "test_sequence = np.random.randint(1, input_vocab_size, (1, input_length))  # Single test sequence\n",
    "test_sequence_onehot = np.eye(input_vocab_size)[test_sequence]  # One-hot encode the input\n",
    "decoded_output, att_weights = decode_sequence(test_sequence_onehot)\n",
    "\n",
    "# Print the results\n",
    "print(\"Input sequence:\", test_sequence[0])  # Original input sequence\n",
    "print(\"Decoded sequence:\", decoded_output)  # Reversed sequence predicted by the model\n",
    "\n",
    "# Visualize attention weights\n",
    "plot_attention(test_sequence, att_weights, decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above heatmap shows how the attention weights changed for each decoding step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsiis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
