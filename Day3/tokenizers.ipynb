{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "corpus = \"\"\"Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
    "\n",
    "\n",
    "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
    "and other advanced techniques. Happiness is the key to success.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character level tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-Level Tokens (First 50):\n",
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'J', 'o', 'h', 'n', ' ', 'w', 'o', 'r', 'k', 's', ' ', 'a', 't', ' ', 'O', 'p', 'e', 'n', 'A', 'I', ',', ' ', 'l', 'o', 'c', 'a', 't', 'e', 'd', ' ', 'i', 'n', ' ', 'S', 'a', 'n', ' ']\n",
      "\n",
      "Total Character Tokens: 267\n"
     ]
    }
   ],
   "source": [
    "# Character-level tokenization\n",
    "char_tokens = list(corpus)\n",
    "\n",
    "# Display the first 50 characters as tokens\n",
    "print(\"Character-Level Tokens (First 50):\")\n",
    "print(char_tokens[:50])\n",
    "\n",
    "# Display the total number of character tokens\n",
    "print(\"\\nTotal Character Tokens:\", len(char_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-Level Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-Level Tokens (First 20):\n",
      "['Hello', 'world', 'John', 'works', 'at', 'OpenAI', 'located', 'in', 'San', 'Francisco', 'His', 'email', 'is', 'john', 'doe', 'example', 'com', 'He', 'recently', 'completed']\n",
      "\n",
      "Total Word Tokens: 44\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Word-level tokenization using regex\n",
    "word_tokens = re.findall(r'\\b\\w+\\b', corpus)\n",
    "\n",
    "# Display the first 20 word tokens\n",
    "print(\"Word-Level Tokens (First 20):\")\n",
    "print(word_tokens[:20])\n",
    "\n",
    "# Display the total number of word tokens\n",
    "print(\"\\nTotal Word Tokens:\", len(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-Level Tokens:\n",
      "Sentence 1: Hello world!\n",
      "Sentence 2: John works at OpenAI, located in San Francisco.\n",
      "Sentence 3: His email is john.doe@example.com.\n",
      "Sentence 4: \n",
      "\n",
      "\n",
      "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
      "and other advanced techniques.\n",
      "Sentence 5: Happiness is the key to success.\n",
      "\n",
      "Total Sentences: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sentence-level tokenization using regular expressions\n",
    "sentence_tokens = re.split(r'(?<=[.!?]) +', corpus)\n",
    "\n",
    "# Display the sentence tokens\n",
    "print(\"Sentence-Level Tokens:\")\n",
    "for idx, sentence in enumerate(sentence_tokens):\n",
    "    print(f\"Sentence {idx + 1}: {sentence}\")\n",
    "\n",
    "# Display the total number of sentences\n",
    "print(\"\\nTotal Sentences:\", len(sentence_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paragraph-Level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph-Level Tokens:\n",
      "Paragraph 1: Hello world! John works at OpenAI, located in San Francisco. His email is john.doe@example.com. \n",
      "Paragraph 2: \n",
      "He recently completed a project on tokenization on 12/12/2023, focusing on character-level, word-level, \n",
      "and other advanced techniques. Happiness is the key to success.\n",
      "\n",
      "Total Paragraphs: 2\n"
     ]
    }
   ],
   "source": [
    "# Paragraph-level tokenization by splitting on double newlines\n",
    "paragraph_tokens = corpus.split('\\n\\n')\n",
    "\n",
    "# Display the paragraph tokens\n",
    "print(\"Paragraph-Level Tokens:\")\n",
    "for idx, paragraph in enumerate(paragraph_tokens):\n",
    "    print(f\"Paragraph {idx + 1}: {paragraph}\")\n",
    "\n",
    "# Display the total number of paragraphs\n",
    "print(\"\\nTotal Paragraphs:\", len(paragraph_tokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams (n=2):\n",
      "('Hello', 'world!')\n",
      "('world!', 'John')\n",
      "('John', 'works')\n",
      "('works', 'at')\n",
      "('at', 'OpenAI,')\n",
      "('OpenAI,', 'located')\n",
      "('located', 'in')\n",
      "('in', 'San')\n",
      "('San', 'Francisco.')\n",
      "('Francisco.', 'His')\n",
      "('His', 'email')\n",
      "('email', 'is')\n",
      "('is', 'john.doe@example.com.')\n",
      "('john.doe@example.com.', 'He')\n",
      "('He', 'recently')\n",
      "('recently', 'completed')\n",
      "('completed', 'a')\n",
      "('a', 'project')\n",
      "('project', 'on')\n",
      "('on', 'tokenization')\n",
      "('tokenization', 'on')\n",
      "('on', '12/12/2023,')\n",
      "('12/12/2023,', 'focusing')\n",
      "('focusing', 'on')\n",
      "('on', 'character-level,')\n",
      "('character-level,', 'word-level,')\n",
      "('word-level,', 'and')\n",
      "('and', 'other')\n",
      "('other', 'advanced')\n",
      "('advanced', 'techniques.')\n",
      "('techniques.', 'Happiness')\n",
      "('Happiness', 'is')\n",
      "('is', 'the')\n",
      "('the', 'key')\n",
      "('key', 'to')\n",
      "('to', 'success.')\n",
      "\n",
      "Trigrams (n=3):\n",
      "('Hello', 'world!', 'John')\n",
      "('world!', 'John', 'works')\n",
      "('John', 'works', 'at')\n",
      "('works', 'at', 'OpenAI,')\n",
      "('at', 'OpenAI,', 'located')\n",
      "('OpenAI,', 'located', 'in')\n",
      "('located', 'in', 'San')\n",
      "('in', 'San', 'Francisco.')\n",
      "('San', 'Francisco.', 'His')\n",
      "('Francisco.', 'His', 'email')\n",
      "('His', 'email', 'is')\n",
      "('email', 'is', 'john.doe@example.com.')\n",
      "('is', 'john.doe@example.com.', 'He')\n",
      "('john.doe@example.com.', 'He', 'recently')\n",
      "('He', 'recently', 'completed')\n",
      "('recently', 'completed', 'a')\n",
      "('completed', 'a', 'project')\n",
      "('a', 'project', 'on')\n",
      "('project', 'on', 'tokenization')\n",
      "('on', 'tokenization', 'on')\n",
      "('tokenization', 'on', '12/12/2023,')\n",
      "('on', '12/12/2023,', 'focusing')\n",
      "('12/12/2023,', 'focusing', 'on')\n",
      "('focusing', 'on', 'character-level,')\n",
      "('on', 'character-level,', 'word-level,')\n",
      "('character-level,', 'word-level,', 'and')\n",
      "('word-level,', 'and', 'other')\n",
      "('and', 'other', 'advanced')\n",
      "('other', 'advanced', 'techniques.')\n",
      "('advanced', 'techniques.', 'Happiness')\n",
      "('techniques.', 'Happiness', 'is')\n",
      "('Happiness', 'is', 'the')\n",
      "('is', 'the', 'key')\n",
      "('the', 'key', 'to')\n",
      "('key', 'to', 'success.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Sample text\n",
    "#text = \"He recently completed a project on tokenization, focusing on character-level, word-level, and morphological analysis.\"\n",
    "\n",
    "# Split the text into words\n",
    "words = corpus.split()\n",
    "\n",
    "# Generate bigrams (n=2)\n",
    "bigrams = list(ngrams(words, 2))\n",
    "print(\"Bigrams (n=2):\")\n",
    "for bigram in bigrams:\n",
    "    print(bigram)\n",
    "\n",
    "# Generate trigrams (n=3)\n",
    "trigrams = list(ngrams(words, 3))\n",
    "print(\"\\nTrigrams (n=3):\")\n",
    "for trigram in trigrams:\n",
    "    print(trigram)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails Found:\n",
      "john.doe@example.com\n",
      "\n",
      "Dates Found:\n",
      "09/30/2023\n",
      "September\n",
      "\n",
      "URLs Found:\n",
      "https://www.example.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Dr. John Doe can be reached at john.doe@example.com. \n",
    "The project deadline is 09/30/2023, and the report is due on September 15, 2023. \n",
    "Visit our website at https://www.example.com for more information.\n",
    "\"\"\"\n",
    "\n",
    "# Regex patterns for various entities\n",
    "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "date_pattern = r'\\b(\\d{2}/\\d{2}/\\d{4})\\b|\\b(January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4}\\b'\n",
    "url_pattern = r'https?://(?:www\\.)?[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}(?:/[a-zA-Z0-9._%+-]*)*'\n",
    "\n",
    "\n",
    "# Extract emails\n",
    "emails = re.findall(email_pattern, text)\n",
    "print(\"Emails Found:\")\n",
    "for email in emails:\n",
    "    print(email)\n",
    "\n",
    "# Extract dates\n",
    "dates = re.findall(date_pattern, text)\n",
    "print(\"\\nDates Found:\")\n",
    "for date in dates:\n",
    "    # date is a tuple with two parts; we only want the non-empty part\n",
    "    print(\"\".join(date))\n",
    "\n",
    "# Extract URLs\n",
    "urls = re.findall(url_pattern, text)\n",
    "print(\"\\nURLs Found:\")\n",
    "for url in urls:\n",
    "    print(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsiis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
